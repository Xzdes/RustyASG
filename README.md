# RustyASG: Графовый Движок для Глубокого Обучения на Rust

**RustyASG** — это экспериментальный фреймворк для глубокого обучения, написанный на Rust. Его ключевая особенность — архитектура, построенная вокруг **Абстрактного Семантического Графа (ASG)**.

Вместо немедленного выполнения операций (eager execution), как в PyTorch, RustyASG использует подход "сначала определи, потом запусти" (define-then-run), аналогичный классическому TensorFlow. Сначала строится полный граф вычислений, который затем может быть проанализирован, оптимизирован и выполнен на различных бэкендах.

Этот проект является демонстрацией того, как такая система может быть спроектирована с нуля, включая динамическое построение графа, автоматическое дифференцирование по принципу "граф-в-граф" и реализацию сложных архитектур, таких как Трансформер.

[![Лицензия: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Ключевые Концепции

Архитектура фреймворка разделена на несколько ключевых, независимых модулей:

*   **`asg` (Ядро Графа):** Сердце фреймворка. Здесь определены основные структуры: `Asg` (сам граф), `Node` (узел данных или операции) и `NodeType` (полный перечень всех возможных операций). Этот модуль ничего не вычисляет, он только описывает *структуру* вычислений.

*   **`tensor` (Символьный API):** Пользовательский интерфейс для построения графа. `Tensor` в этом фреймворке — это не контейнер для данных, а легковесный "дескриптор" или символьная переменная, которая ссылается на узел в графе. Любая операция над `Tensor` (`a + b`, `a.dot(b)`) не выполняет вычисление, а добавляет новый узел в ASG.

*   **`autograd` (Автоматическое Дифференцирование):** "Мозг" системы. Этот модуль реализует преобразование "граф-в-граф". Он принимает на вход ASG, который вычисляет функцию (например, потери), и генерирует на выходе **новый ASG**, который описывает, как вычислить производные этой функции по отношению к заданным переменным.

*   **`runtime` (Исполнитель):** "Мышцы" системы. Этот модуль берет ASG и реальные числовые данные и выполняет вычисления. Текущая реализация — это простой `interpreter`, который последовательно обходит граф и выполняет операции на CPU с помощью `ndarray`.

*   **`nn` (Слои Нейронных Сетей):** Высокоуровневые строительные блоки (`Linear`, `LayerNorm`, `MultiHeadAttention`, `TransformerBlock`). Каждый слой — это конструктор, который добавляет в граф определенный паттерн узлов и параметров.

*   **`optimizers` (Оптимизаторы):** Алгоритмы, которые используют вычисленные градиенты для обновления реальных весов модели. Работают с числовыми данными, а не с графом.

## Текущий Статус: РАБОТАЕТ!

Проект успешно прошел путь от нерабочего прототипа до функционального мини-фреймворка.

*   ✅ **Полный цикл обучения:** Демонстрация в `main.rs` успешно строит модель `TransformerBlock`, вычисляет потери, строит граф градиентов, выполняет оба графа и обновляет веса с помощью `SGD`.
*   ✅ **Обучение происходит:** Значение функции потерь (`Loss`) стабильно уменьшается на протяжении эпох, что доказывает корректность работы всего графового движка и механизма обратного распространения ошибки для большинства операций.
*   ✅ **Сложные архитектуры:** Реализованы и работают `MultiHeadAttention` и `TransformerBlock`.

#### Известные Ограничения:
*   **Градиент для `LayerNorm`:** Градиент для параметров `LayerNorm` (`gamma` и `beta`) в `autograd` реализован некорректно. В демонстрационном примере их обновление временно отключено, чтобы не мешать обучению остальной части сети. **Это главная задача для дальнейшей работы.**
*   **Один выход градиента:** Текущая реализация `autograd` строит граф, который вычисляет градиент только для одного параметра за раз. В `main.rs` это обходится путем построения нескольких графов в цикле.

## Как Запустить

Для запуска проекта вам понадобится установленный Rust и Cargo.

1.  **Клонируйте репозиторий:**
    ```bash
    git clone https://github.com/Xzdes/RustyAsg.git
    cd RustyAsg
    ```

2.  **Запустите демонстрацию:**
    ```bash
    cargo run
    ```

3.  **Ожидаемый результат:**
    Вы увидите в консоли лог, показывающий построение графов и запуск цикла обучения, в котором значение потерь будет уменьшаться с каждой эпохой:

    ```
    --- Демонстрация полного цикла обучения RustyGradients ---

    Граф прямого прохода и вычисления потерь успешно построен.
    Данные и веса инициализированы.

    --- НАЧАЛО ЦИКЛА ОБУЧЕНИЯ ---

    Эпоха: 1 , Потери (Loss): 4.857111
    Эпоха: 2 , Потери (Loss): 4.428864
    Эпоха: 3 , Потери (Loss): 4.045469
    ...
    Эпоха: 14, Потери (Loss): 1.293629
    Эпоха: 15, Потери (Loss): 1.142278

    --- ОБУЧЕНИЕ ЗАВЕРШЕНО ---
    ```

## Структура Проекта

```
src/
├── asg/          # Ядро: определение структур Asg, Node, NodeType
├── autograd/     # Движок автоматического дифференцирования (граф-в-граф)
├── losses/       # Функции потерь (MSE)
├── nn/           # Слои нейронных сетей (Linear, Attention, TransformerBlock и др.)
├── optimizers/   # Оптимизаторы (SGD)
├── runtime/      # Исполнительные бэкенды (простой CPU интерпретатор)
├── tensor.rs     # Символьный Tensor API для построения графа
└── main.rs       # Файл с демонстрацией полного цикла обучения
```

## Дальнейшие Планы

*   **Исправить градиент для `LayerNorm`**.
*   Расширить `autograd` для поддержки большего числа операций.
*   Реализовать более продвинутые оптимизаторы (`Adam`).
*   Добавить поддержку вывода размерностей (`shape inference`) в графе.
*   Создать GPU-бэкенд для выполнения графов (например, с использованием `wgpu`).

## Лицензия

Этот проект распространяется под лицензией MIT.